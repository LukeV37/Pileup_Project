{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057ddbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e76b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.open(\"../pythia/output/dataset_ttbar_mu60_NumEvents10000_MinJetpT25.root:fastjet\") as f:\n",
    "    jet_pufr = f[\"jet_pufr_truth\"].array()\n",
    "    jet_corrJVF = f[\"jet_corrJVF\"].array()\n",
    "    jet_RpT = f[\"jet_RpT\"].array()\n",
    "    jet_pt = f[\"jet_pt\"].array()\n",
    "    jet_eta = f[\"jet_eta\"].array()\n",
    "    jet_phi = f[\"jet_phi\"].array()\n",
    "    jet_m = f[\"jet_m\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d65416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "sig = (jet_pufr < threshold)\n",
    "bkg = ~sig\n",
    "\n",
    "plt.title(\"Jet Truth Pileup Fraction\")\n",
    "plt.hist(ak.ravel(jet_pufr[sig]),histtype='step',bins=int(100*threshold),label='HS Jets',color='r')\n",
    "plt.hist(ak.ravel(jet_pufr[bkg]),histtype='step',bins=int(100*(1-threshold)),label='PU Jets',color='b')\n",
    "plt.axvline(x = threshold, color = 'k', label = 'PU Jet Cut',linestyle='dashed')\n",
    "plt.xlabel(\"pufr\",loc='right')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d46e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list = [jet_pt,jet_eta,jet_phi,jet_m,jet_corrJVF,jet_RpT]\n",
    "norm_list = []\n",
    "for feat in feat_list:\n",
    "    mean = ak.mean(feat)\n",
    "    std = ak.std(feat)\n",
    "    norm_list.append((feat-mean)/std)\n",
    "feat_list = [x[:,:,np.newaxis] for x in norm_list]\n",
    "jet_feats = ak.concatenate(feat_list, axis=2)\n",
    "\n",
    "threshold = 0.7\n",
    "jet_labels = ak.values_astype((ak.Array(jet_pufr) < threshold), int)\n",
    "\n",
    "num_events = len(jet_pt)\n",
    "train_split = int(0.7*num_events)\n",
    "test_split = int(0.75*num_events)\n",
    "\n",
    "batch_size = 128\n",
    "num_batches = int(train_split / batch_size)-1\n",
    "\n",
    "X_train = jet_feats[0:train_split]\n",
    "y_train = jet_labels[0:train_split][:,:,np.newaxis]\n",
    "\n",
    "batches = []\n",
    "labels = []\n",
    "for i in range(num_batches):\n",
    "    b = torch.nested.nested_tensor(ak.to_list(X_train[i*batch_size:(i+1)*batch_size]), dtype=torch.float32, layout=torch.jagged, requires_grad=True)\n",
    "    l = torch.nested.nested_tensor(ak.to_list(y_train[i*batch_size:(i+1)*batch_size]), dtype=torch.float32, layout=torch.jagged)\n",
    "    batches.append(b)\n",
    "    labels.append(l)\n",
    "    \n",
    "X_train = batches\n",
    "X_val = torch.nested.nested_tensor(ak.to_list(jet_feats[train_split:test_split]), dtype=torch.float32, layout=torch.jagged)\n",
    "X_test = torch.nested.nested_tensor(ak.to_list(jet_feats[test_split:]), dtype=torch.float32, layout=torch.jagged)\n",
    "\n",
    "y_train = labels\n",
    "y_val = torch.nested.nested_tensor(ak.to_list(jet_labels[train_split:test_split]), dtype=torch.int32, layout=torch.jagged)\n",
    "y_test = torch.nested.nested_tensor(ak.to_list(jet_labels[test_split:]), dtype=torch.int32, layout=torch.jagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b1ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes multi-head attention. Supports nested or padded tensors.\n",
    "\n",
    "    Args:\n",
    "        E_q (int): Size of embedding dim for query\n",
    "        E_k (int): Size of embedding dim for key\n",
    "        E_v (int): Size of embedding dim for value\n",
    "        E_total (int): Total embedding dim of combined heads post input projection. Each head\n",
    "            has dim E_total // nheads\n",
    "        nheads (int): Number of heads\n",
    "        dropout_p (float, optional): Dropout probability. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, E_q: int, E_k: int, E_v: int, E_total: int,\n",
    "                 nheads: int, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.query_proj = nn.Linear(E_q, E_total)\n",
    "        self.key_proj = nn.Linear(E_k, E_total)\n",
    "        self.value_proj = nn.Linear(E_v, E_total)\n",
    "        E_out = E_q\n",
    "        self.out_proj = nn.Linear(E_total, E_out)\n",
    "        assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n",
    "        self.E_head = E_total // nheads\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass; runs the following process:\n",
    "            1. Apply input projection\n",
    "            2. Split heads and prepare for SDPA\n",
    "            3. Run SDPA\n",
    "            4. Apply output projection\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): query of shape (N, L_t, E_q)\n",
    "            key (torch.Tensor): key of shape (N, L_s, E_k)\n",
    "            value (torch.Tensor): value of shape (N, L_s, E_v)\n",
    "\n",
    "        Returns:\n",
    "            attn_output (torch.Tensor): output of shape (N, L_t, E_q)\n",
    "        \"\"\"\n",
    "        # Step 1. Apply input projection\n",
    "        # TODO: demonstrate packed projection\n",
    "        query = self.query_proj(query)\n",
    "        key = self.key_proj(key)\n",
    "        value = self.value_proj(value)\n",
    "\n",
    "        # Step 2. Split heads and prepare for SDPA\n",
    "        # reshape query, key, value to separate by head\n",
    "        # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)\n",
    "        query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "\n",
    "        # Step 3. Run SDPA\n",
    "        # (N, nheads, L_t, E_head)\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            query, key, value, dropout_p=self.dropout_p, is_causal=False)\n",
    "        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n",
    "        attn_output = attn_output.transpose(1, 2).flatten(-2)\n",
    "\n",
    "        # Step 4. Apply output projection\n",
    "        # (N, L_t, E_total) -> (N, L_t, E_out)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.attention = nn.MultiheadAttention(embed_dim,num_heads=num_heads,batch_first=True, dropout=0.25)\n",
    "        self.Attention = MultiHeadAttention(32,32,32,32,1)\n",
    "        self.out = nn.Linear(embed_dim,embed_dim)\n",
    "    def forward(self, Query, Key, Value):\n",
    "        #context, weights = self.attention(Query, Key, Value)\n",
    "        context = self.Attention(Query, Key, Value)\n",
    "        latent = Query + context\n",
    "        tmp = F.gelu(self.out(latent))\n",
    "        latent = latent + tmp\n",
    "        return latent\n",
    "\n",
    "class AttnModel(nn.Module):\n",
    "    def __init__(self, in_feats, embed_dim, num_heads, out_classes):\n",
    "        super(AttnModel, self).__init__()        \n",
    "        self.preprocess = nn.Linear(in_feats,embed_dim)\n",
    "        self.encoder1 = Encoder(embed_dim, num_heads)\n",
    "        self.classifier = nn.Linear(embed_dim,out_classes)\n",
    "    def forward(self, jet_feats):\n",
    "        jet_embedding = F.gelu(self.preprocess(jet_feats))\n",
    "        jet_embedding = self.encoder1(jet_embedding,jet_embedding,jet_embedding)\n",
    "        output = F.sigmoid(self.classifier(jet_embedding))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ead105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE_Loss(preds, targets):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    loss = 0.0\n",
    "    for pred, target in zip(preds.unbind(), targets.unbind()):\n",
    "        #print(len(pred))\n",
    "        loss += loss_fn(pred, target)\n",
    "    print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146f7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs=20):\n",
    "    X_train, y_train, X_val, y_val = data\n",
    "    \n",
    "    history = {'train_loss':[],'val_loss':[]}\n",
    "   \n",
    "    for e in range(epochs):     \n",
    "        cum_loss_train = []\n",
    "        cum_loss_val = []\n",
    "\n",
    "        num_batches = len(X_train)\n",
    "        model.train()\n",
    "        for i in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            padded_X_train = torch.nested.to_padded_tensor(X_train[i], padding=-999)\n",
    "            padded_y_train = torch.nested.to_padded_tensor(y_train[i], padding=-999)\n",
    "            \n",
    "            output = model(padded_X_train.to(device))\n",
    "      \n",
    "            loss = BCE_Loss(output, padded_y_train.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cum_loss_train.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        model.eval()\n",
    "        y_pred = model(X_val.to(device))\n",
    "        loss = BCE_Loss(y_pred,y_val.to(device))\n",
    "        cum_loss_val.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        history['train_loss'].append(np.mean(cum_loss_train))\n",
    "        history['val_loss'].append(np.mean(cum_loss_val))\n",
    "\n",
    "        if (e+1)%1==0:\n",
    "            print('Epoch:',e+1,'\\tTrain Loss:',round(float(np.mean(cum_loss_train)),4),'\\tVal Loss:',round(float(np.mean(cum_loss_val)),4))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e10f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dffa8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NestedTensor(size=(128, j1, 1), offsets=tensor([   0,   22,   71,   96,  126,  166,  205,  278,  312,  343,  383,  441,\n",
      "         509,  568,  632,  680,  712,  741,  770,  808,  869,  883,  946,  998,\n",
      "        1076, 1099, 1139, 1165, 1232, 1266, 1321, 1403, 1414, 1437, 1490, 1581,\n",
      "        1632, 1651, 1697, 1752, 1795, 1834, 1905, 1942, 1988, 2025, 2066, 2165,\n",
      "        2207, 2262, 2289, 2343, 2394, 2471, 2518, 2565, 2612, 2659, 2714, 2751,\n",
      "        2767, 2808, 2843, 2865, 2918, 2953, 2987, 3015, 3045, 3092, 3125, 3185,\n",
      "        3234, 3280, 3320, 3387, 3412, 3449, 3467, 3511, 3578, 3603, 3669, 3687,\n",
      "        3737, 3814, 3866, 3889, 3943, 4009, 4035, 4074, 4156, 4230, 4240, 4275,\n",
      "        4310, 4340, 4353, 4382, 4408, 4445, 4482, 4515, 4559, 4630, 4664, 4691,\n",
      "        4754, 4764, 4780, 4817, 4849, 4882, 4902, 4936, 4994, 5043, 5085, 5126,\n",
      "        5195, 5221, 5257, 5306, 5354, 5425, 5444, 5470, 5516], device='cuda:0'), grad_fn=<SigmoidBackward0 object at 0x774c955cf820>, contiguous=True)\n",
      "NestedTensor(size=(128, j2, 1), offsets=tensor([   0,   22,   71,   96,  126,  166,  205,  278,  312,  343,  383,  441,\n",
      "         509,  568,  632,  680,  712,  741,  770,  808,  869,  883,  946,  998,\n",
      "        1076, 1099, 1139, 1165, 1232, 1266, 1321, 1403, 1414, 1437, 1490, 1581,\n",
      "        1632, 1651, 1697, 1752, 1795, 1834, 1905, 1942, 1988, 2025, 2066, 2165,\n",
      "        2207, 2262, 2289, 2343, 2394, 2471, 2518, 2565, 2612, 2659, 2714, 2751,\n",
      "        2767, 2808, 2843, 2865, 2918, 2953, 2987, 3015, 3045, 3092, 3125, 3185,\n",
      "        3234, 3280, 3320, 3387, 3412, 3449, 3467, 3511, 3578, 3603, 3669, 3687,\n",
      "        3737, 3814, 3866, 3889, 3943, 4009, 4035, 4074, 4156, 4230, 4240, 4275,\n",
      "        4310, 4340, 4353, 4382, 4408, 4445, 4482, 4515, 4559, 4630, 4664, 4691,\n",
      "        4754, 4764, 4780, 4817, 4849, 4882, 4902, 4936, 4994, 5043, 5085, 5126,\n",
      "        5195, 5221, 5257, 5306, 5354, 5425, 5444, 5470, 5516]), contiguous=True)\n"
     ]
    }
   ],
   "source": [
    "model = AttnModel(6,32,1,1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "data = [X_train, y_train, X_val, y_val]\n",
    "\n",
    "#history = train(model, data)\n",
    "\n",
    "print(model(X_train[0].to(device)))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75130228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Check gradients\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients for a:\u001b[39m\u001b[38;5;124m\"\u001b[39m, a\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example nested tensors with requires_grad=True\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 5.0, 6.0, 7.0], requires_grad=True)\n",
    "nested_tensor = torch.nested.nested_tensor([a, b])\n",
    "\n",
    "# Example target nested tensor\n",
    "target_a = torch.tensor([1.0, 2.0, 3.0])\n",
    "target_b = torch.tensor([4.0, 5.0, 6.0, 7.0])\n",
    "target_nested_tensor = torch.nested.nested_tensor([target_a, target_b])\n",
    "\n",
    "# Unbind the nested tensor\n",
    "unbound_tensors = nested_tensor.unbind()\n",
    "target_unbound_tensors = target_nested_tensor.unbind()\n",
    "\n",
    "# Compute the loss for each tensor\n",
    "def compute_loss(unbound_tensors, target_unbound_tensors):\n",
    "    total_loss = 0.0\n",
    "    for pred, target in zip(unbound_tensors, target_unbound_tensors):\n",
    "        total_loss += F.mse_loss(pred, target)\n",
    "    return total_loss\n",
    "\n",
    "# Calculate the total loss\n",
    "loss = compute_loss(unbound_tensors, target_unbound_tensors)\n",
    "print(\"Total Loss:\", loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "print(\"Gradients for a:\", a.grad)\n",
    "print(\"Gradients for b:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "750c89e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0\n",
      "Gradients for a: None\n",
      "Gradients for b: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example nested tensors\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 5.0, 6.0, 7.0], requires_grad=True)\n",
    "nested_tensor = torch.nested.nested_tensor([a, b], requires_grad=True)\n",
    "\n",
    "# Convert to padded tensor\n",
    "padded_tensor = torch.nested.to_padded_tensor(nested_tensor, padding=0.0)\n",
    "\n",
    "# Example target tensor (also padded)\n",
    "target = torch.tensor([[1.0, 2.0, 3.0, 0.0], [4.0, 5.0, 6.0, 7.0]])\n",
    "\n",
    "# Create a mask\n",
    "mask = (padded_tensor != 0.0).float()\n",
    "\n",
    "# Compute loss with masking\n",
    "loss = torch.nn.functional.mse_loss(padded_tensor * mask, target * mask)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "print(\"Gradients for a:\", a.grad)\n",
    "print(\"Gradients for b:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c6842d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 22.53557014465332\n",
      "Gradients for a: None\n",
      "Gradients for b: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example nested tensors\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 5.0, 6.0, 7.0], requires_grad=True)\n",
    "nested_tensor = torch.nested.nested_tensor([a, b], requires_grad=True)\n",
    "\n",
    "# Convert to padded tensor\n",
    "padded_tensor = torch.nested.to_padded_tensor(nested_tensor, padding=0.0)\n",
    "\n",
    "# Define MHA layer\n",
    "mha = torch.nn.MultiheadAttention(embed_dim=4, num_heads=2, batch_first=True)\n",
    "\n",
    "# Example query, key, value tensors\n",
    "query = padded_tensor.unsqueeze(0)  # Add batch dimension\n",
    "key = padded_tensor.unsqueeze(0)\n",
    "value = padded_tensor.unsqueeze(0)\n",
    "\n",
    "# Pass through MHA layer\n",
    "attn_output, _ = mha(query, key, value)\n",
    "\n",
    "# Create a mask\n",
    "mask = (padded_tensor != 0.0).float()\n",
    "\n",
    "# Example target tensor (also padded)\n",
    "target = torch.tensor([[1.0, 2.0, 3.0, 0.0], [4.0, 5.0, 6.0, 7.0]])\n",
    "\n",
    "# Compute loss with masking\n",
    "loss = F.mse_loss(attn_output.squeeze(0) * mask, target * mask)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "print(\"Gradients for a:\", a.grad)\n",
    "print(\"Gradients for b:\", b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed103b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
